# Integration Testing Patterns for MCP Servers

## Context

This document captures key patterns and lessons learned from implementing integration tests for the MD-GTD-MCP server, specifically from task 5.1 (new user onboarding workflow). These patterns can be applied to other MCP server projects and the remaining integration tests in this specification.

## Core Integration Testing Principles

### 1. Test Complete User Workflows, Not Just Individual Components

**Pattern**: Structure tests around end-to-end user journeys rather than isolated function calls.

```python
def test_complete_gtd_vault_setup_from_empty_directory(self) -> None:
    """Test new user onboarding workflow - Complete GTD vault setup from empty directory."""
    # Step 1: Setup (vault creation)
    # Step 2: Validation (structure verification)
    # Step 3: Integration (VaultReader compatibility)
    # Step 4: Content Quality (template validation)
```

**Why**: MCP servers are primarily about enabling AI workflows. Testing the complete user journey ensures the MCP tools work together cohesively and provide value to end users.

### 2. Use Isolated Temporary Environments

**Pattern**: Create fresh, isolated test environments for each test using `tempfile.TemporaryDirectory()`.

```python
with tempfile.TemporaryDirectory() as temp_dir:
    vault_path = Path(temp_dir) / "test_vault"
    # Test implementation with guaranteed cleanup
```

**Benefits**:
- No test pollution between runs
- Safe from accidentally modifying real user data
- Automatic cleanup on test completion
- Reproducible test conditions

### 3. Validate Data Safety First

**Pattern**: Explicitly test that existing data is never overwritten or corrupted.

```python
# Verify all expected files were created
expected_created = {...}
created_set = set(setup_result["created"])
assert expected_created.issubset(created_set)
assert len(setup_result["already_existed"]) == 0  # Nothing should pre-exist
```

**Critical for GTD/Productivity Systems**: Users' productivity data is irreplaceable. Tests must validate that operations are safe before testing functionality.

### 4. Test MCP Protocol and Service Layer Integration

**Pattern**: Test both the MCP tool interface and the underlying service implementation in the same test.

```python
# Test MCP tool
setup_result = setup_gtd_vault(str(vault_path))  # MCP tool call

# Test service layer integration
vault_config = VaultConfig(vault_path)
vault_reader = VaultReader(vault_config)  # Service layer
all_files = vault_reader.read_all_gtd_files()
```

**Why**: Ensures the MCP tools correctly integrate with the underlying business logic and that the service layer can handle MCP-generated data.

## File System Testing Patterns

### 1. Template Content Validation

**Pattern**: Validate both structure and semantic content of generated files.

```python
# Structure validation
assert len(all_files) == 9  # Expected file count
assert expected_types == file_types  # File type coverage

# Content validation
assert "# Inbox" in inbox.content
assert "capture everything here first" in inbox.content.lower()
```

**Insight**: Don't just test that files exist - validate they contain meaningful, usable content.

### 2. Cross-Component Integration Testing

**Pattern**: Test that different parsers and readers work correctly with MCP-generated content.

```python
# Generated by MCP setup tool
setup_result = setup_gtd_vault(str(vault_path))

# Read by VaultReader service
all_files = vault_reader.read_all_gtd_files()

# Parsed by individual parsers (tested implicitly)
for context_file in context_files:
    assert "```tasks" in context_file.content  # TaskExtractor compatibility
    assert len(context_file.tasks) == 0  # Correct parsing behavior
```

**Why**: MCP servers often involve multiple parsing and processing components. Integration tests ensure they work together correctly.

### 3. File Type Classification Testing

**Pattern**: Validate that the file type detection logic works correctly with MCP-generated files.

```python
file_types = {f.file_type for f in all_files}
expected_types = {"inbox", "projects", "next-actions", "waiting-for", "someday-maybe", "context"}
assert expected_types == file_types

# Specific file type validation
context_files = [f for f in all_files if f.file_type == "context"]
assert len(context_files) == 4
```

**Insight**: File type detection is often path-based. Test that MCP-generated paths are correctly categorized.

## GTD-Specific Testing Patterns

### 1. Template vs. User Data Distinction

**Pattern**: Ensure template files don't contain tasks/user data initially.

```python
# Templates should not contain any tasks initially
for gtd_file in all_files:
    if gtd_file.file_type != "context":  # Context files have query blocks, not tasks
        assert len(gtd_file.tasks) == 0
```

**Why**: GTD systems need clear separation between template structure and user-generated content.

### 2. Context File Query Syntax Validation

**Pattern**: Validate that context files contain proper Obsidian Tasks query syntax.

```python
for context_file in context_files:
    assert "```tasks" in context_file.content
    assert "not done" in context_file.content
    assert "```" in context_file.content

    # Context-specific validation
    if "@calls" in context_file.path:
        assert "ðŸ“ž" in context_file.content
        assert "@calls" in context_file.content
```

**Insight**: Context files in GTD systems often contain query syntax rather than actual tasks. Test the syntax, not task extraction.

### 3. GTD Workflow Readiness Testing

**Pattern**: Validate that generated files are immediately usable for GTD workflows.

```python
# All template files should be valid markdown
for gtd_file in all_files:
    assert gtd_file.content is not None
    assert len(gtd_file.content.strip()) > 0
    assert gtd_file.title is not None
```

**Why**: Generated templates should be production-ready, not just structurally correct.

## Test Organization Patterns

### 1. Workflow-Based Test Classes

**Pattern**: Organize tests by user workflows rather than technical components.

```python
class TestNewUserOnboardingWorkflow:
    """Integration tests for task 5.1: New user onboarding workflow."""

class TestExistingUserMigrationWorkflow:
    """Integration tests for task 5.2: Existing user migration workflow."""
```

**Benefits**:
- Tests are organized by user value
- Easy to understand test coverage
- Aligns with task breakdown structure

### 2. Progressive Validation Steps

**Pattern**: Structure tests with clear, numbered validation steps.

```python
# Step 1: Setup GTD vault structure
# Step 2: Test list_gtd_files shows proper structure
# Step 3: Verify read_gtd_files returns expected templates
# Step 4: Verify files are ready for immediate use
```

**Why**: Makes test failures easier to diagnose and ensures comprehensive coverage.

### 3. Assertion Grouping

**Pattern**: Group related assertions with explanatory comments.

```python
# Verify all expected files were created
expected_created = {...}
created_set = set(setup_result["created"])
assert expected_created.issubset(created_set)
assert len(setup_result["already_existed"]) == 0  # Nothing should pre-exist
```

**Benefits**: Clear test intent and easier debugging when assertions fail.

## Lessons from Task 5.1 Implementation

### 1. Template Content Evolution

**Learning**: Template content changed during development. Tests initially failed because they expected "desired outcome" but templates contained "defined outcomes".

**Pattern**: Use flexible content assertions that focus on key concepts rather than exact text:

```python
# Flexible - focuses on concept
assert "defined outcomes" in projects.content.lower()
# Brittle - exact text match
assert "Projects with defined outcomes that require multiple steps." in projects.content
```

### 2. Import Organization Matters

**Learning**: Ruff import sorting rules must be followed for clean integration with existing codebase.

**Pattern**: Order imports consistently:
```python
# Standard library imports
import tempfile
from pathlib import Path

# Third-party imports (if any)

# Local application imports
from md_gtd_mcp.models.vault_config import VaultConfig
from md_gtd_mcp.services.vault_reader import VaultReader
from md_gtd_mcp.services.vault_setup import setup_gtd_vault
from tests.fixtures import create_sample_vault
```

### 3. Line Length and Docstring Formatting

**Learning**: Long test names and docstrings can violate line length limits.

**Solution**: Split long docstrings appropriately:
```python
def test_complete_gtd_vault_setup_from_empty_directory(self) -> None:
    """Test new user onboarding workflow.

    Complete GTD vault setup from empty directory.

    This test verifies:
    - setup_gtd_vault creates all required files/folders
    - list_gtd_files shows proper structure
    - read_gtd_files returns expected templates
    """
```

### 4. Test Isolation Best Practices

**Learning**: Each integration test should be completely independent and not rely on shared state.

**Pattern**: Use fresh temporary directories and avoid class-level setup:
```python
def test_workflow(self) -> None:
    with tempfile.TemporaryDirectory() as temp_dir:
        # Fresh environment for this test only
        vault_path = Path(temp_dir) / "test_vault"
        # ... test implementation
```

## Recommendations for Future Integration Tests

### 1. Test Data Safety Early and Often

Always validate that operations preserve existing user data before testing functionality.

### 2. Use Realistic Test Scenarios

Create test scenarios that match real user workflows, not just edge cases.

### 3. Validate MCP Protocol Compliance

Ensure MCP tools return proper JSON structures and handle errors gracefully.

### 4. Test Performance Implications

For productivity systems, test with realistic data volumes to ensure acceptable performance.

### 5. Document Test Intent Clearly

Use descriptive test names and comprehensive docstrings to explain what user workflow is being validated.

## Conclusion

Integration testing for MCP servers requires balancing protocol compliance, service functionality, and user workflow validation. The patterns documented here provide a foundation for building comprehensive test suites that ensure MCP servers deliver reliable, safe, and valuable functionality to end users.

The key insight from task 5.1 is that MCP servers are middleware - they must work correctly at both the protocol level (for AI assistants) and the domain level (for end users). Effective integration tests validate both layers simultaneously.
